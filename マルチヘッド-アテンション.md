# Note
```
guid: yQlxO3ql3E
notetype: AI-Vocabulary-Style
```

### Tags
```
```

## Front
マルチヘッド アテンション

## Back-EN
Multi-head attention

## Back-FR
Attention multiples

## Reading
マルチヘッド アテンション

## Sentence
(Wq, Wk, Wv) 行列の 1 セットがアテンションヘッドと呼ばれ、Transformer モデルの各層には複数のアテンションヘッドがある。 1つのアテンションヘッドは各トークンに関連するトークンに注意を向けるが、複数のアテンションヘッドがあると、モデルは様々な定義の「関連性」に対して同様の操作を行うことができる。Transformer の多くのアテンションヘッドは、人間が解釈可能な関連性関係をエンコードしていることが研究によって示された。
Transformer モデルには複数のアテンションヘッドがあるため、表層レベルからセマンティックまで、多くのレベルと関連性関係の型をとらえることができる。 マルチヘッドアテンション層からの複数の出力は、連結されてフィードフォワードニューラルネットワーク層に渡される。
