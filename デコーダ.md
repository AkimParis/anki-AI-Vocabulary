# Note
```
guid: Q1g(CplHl}
notetype: AI-Vocabulary-Style
```

### Tags
```
```

## Front
デコーダ

## Back-EN
Decoder

## Back-FR
Décodeur

## Reading


## Sentence
各デコーダは、セルフ・アテンション機構、エンコーディングに関するアテンション機構、FFサブレイヤの3コンポーネントで構成されている。 デコーダはエンコーダと同様に機能する、エンコーダによって生成されたエンコーディングから関連情報を引き出す追加のアテンション機構が挿入される。最初のエンコーダと同様に、最初のデコーダは、エンコーディングではなく、位置情報と出力シーケンスの埋め込みを入力として受け取る。 ただし、Transformer は現在または将来の出力を使用して出力を予測するべきではないため、この逆の情報フローを防ぐために出力シーケンスを部分的にマスクする必要がある。 最後のデコーダーの後には、最終的な線形変換とsoftmax層が続き、語彙に対する出力確率が生成される。
