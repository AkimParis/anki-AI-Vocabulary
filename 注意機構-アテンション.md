# Note
```
guid: d_3$*dxZa2
notetype: AI-Vocabulary-Style
```

### Tags
```
```

## Front
注意機構 / アテンション

## Back-EN
Attention mechanism

## Back-FR
Mécanisme d'attention

## Reading
ちゅういきこう / アテンション

## Sentence
すべてのトークンのアテンションの計算は、1つの大きな行列計算として表現できる。これは、行列演算を高速に実行できる行列演算最適化を用いたトレーニングに役立つ。行列 Q、K、V の i 番目の行は、それぞれ、クエリベクトル Qi、キーベクトル Ki 、値ベクトル Vi に相当する。
